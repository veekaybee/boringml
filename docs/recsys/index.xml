<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Boring Machine Learning</title><link>http://boringml.com/docs/recsys/</link><description>Recent content on Boring Machine Learning</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="http://boringml.com/docs/recsys/index.xml" rel="self" type="application/rss+xml"/><item><title>Implementing Minhash</title><link>http://boringml.com/docs/recsys/minhash/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://boringml.com/docs/recsys/minhash/</guid><description>Chapter 3 of Mining Massive DataSets # Chapter Slides
Motivating the Chapter
We cover the first part of this chapter, which deals with Jaccard similarity, shingling, and minhash.
Often these days data analysis involves datasets that have high dimensionality, meaning the data set in question has more features than values and to make statistically sound inferences at scale, require large amounts of data (more info here, on p. 22), and it’s these kinds of datasets that Chapter 3 deals with.</description></item><item><title/><link>http://boringml.com/docs/recsys/jaccard-similarity/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://boringml.com/docs/recsys/jaccard-similarity/</guid><description>Jaccard Similarity # Implemented two different ways:
import numpy as numpy import typing a = [1,2,3,4,5,11,12] b = [2,3,4,5,6,8,9] cats = [&amp;#34;calico&amp;#34;, &amp;#34;tabby&amp;#34;, &amp;#34;tom&amp;#34;] dogs = [&amp;#34;collie&amp;#34;, &amp;#34;tom&amp;#34;,&amp;#34;bassett&amp;#34;] def jaccard(list1: list, list2: list)-&amp;gt; float: intersection = len(list(set(list1).intersection(list2))) union = (len(set((list1)) + set(len(list2))) - intersection return float(intersection/union) print(jaccard(cats,dogs)) jaccardSimilarity in Scala # val aVals: Seq[Int] = Seq(1,2,3,4,5,11,12) val bVals: Seq[Int] = Seq(2,3,4,5,6,8,9) def calculateJaccard[T](a: Seq[T], b: Seq[T]): Double = a.</description></item><item><title>Precision and Recall</title><link>http://boringml.com/docs/recsys/precision-and-recall/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://boringml.com/docs/recsys/precision-and-recall/</guid><description>Precision and Recall
Precision and recall are common ways to evaluate the accuracy of your machine learning or information retrieval model. In other contexts, such as statistics, the measurements around these terms is also known as Type I/Type II errors.
Let’s say you’re working on understanding the relevancy of SnakeSearch, a search engine that looks to find relevant Python documentation for you. Let’s say you want to find documents related to Pandas, as you’re getting started with Pandas, the Python software library, and want some information.</description></item></channel></rss>