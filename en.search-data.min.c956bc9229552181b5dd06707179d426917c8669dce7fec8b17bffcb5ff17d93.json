[{"id":0,"href":"/docs/languages/","title":"Languages","section":"Docs","content":" HI # "},{"id":1,"href":"/docs/languages/scala/build-sbt/","title":"build.sbt and build.Scala","section":"Scala","content":" Building Scala code # SBT, along with Maven, is a default way to build Scala applications. build.sbt is the file that defines how your project is built, but sometimes you\u0026rsquo;ll also see build.scala files in specific projects.\nbuild.scala is the more advanced version of the build.sbt file, and often is used for more complicated projects.\nHere\u0026rsquo;s an example of the difference between .sbt and .scala build files:\nbuild.sbt\nname := \u0026#34;hello\u0026#34; version := \u0026#34;1.0\u0026#34; build.scala\nimport sbt._ import Keys._ object Build extends Build { lazy val root = Project(id = \u0026#34;root\u0026#34;, base = file(\u0026#34;.\u0026#34;)).settings( name := \u0026#34;hello\u0026#34;, version := \u0026#34;1.0\u0026#34; ) } From the official docs\nThe recommended approach is to define most settings in a multi-project build.sbt file, and using project/*.scala files for task implementations or to share values, such as keys. The use of .scala files also depends on how comfortable you or your team are with Scala. Here\u0026rsquo;s another great doc on sbt vs Scala files.\nAn important note is that this functionality is deprecated as of sbt 0.13.12. Here\u0026rsquo;s more about how these two files work together\n"},{"id":2,"href":"/docs/languages/java/arrays/","title":"is_set","section":"Java","content":" Is set versus array_key_exists # I dug into the performance of isset versus array_key_exists in PHP.\nBoth look at an array and determine if it has a specific key, but their behavior is different.\nisset will return false if the value of that key is null. array_key_exists will only look at the key itself.\nUsing an id as an example:\n$a = array(\u0026#39;1\u0026#39; =\u0026gt; \u0026#39;`12345678`\u0026#39;, \u0026#39;key2\u0026#39; =\u0026gt; null); isset($a[\u0026#39;key1\u0026#39;]); //true isset($a[\u0026#39;key2\u0026#39;]); //false array_key_exists(\u0026#39;key1\u0026#39;, $a); // true array_key_exists(\u0026#39;key2\u0026#39;, $a); //true So if you want your function to be null-safe, isset is always the best best. It\u0026rsquo;s also marginally better for performance because it\u0026rsquo;s a PHP language construct rather than a function, like [array_key_exists].\nLooking at a small sample of data, we can see the marginal performance improvements at scale: (note, it\u0026rsquo;s better not to do all three performance comparisons in the same script because, using the same array for inserts means there\u0026rsquo;s a small cache warm-up benefit for array_key_exists if it comes after isset.\n\u0026lt;?php // Small Loops $small_array = array(); for($i = 0;$i \u0026lt; 100;$i++) { array_push($small_array,$i); } $start_small = hrtime(true); for($i = 0;$i \u0026lt; 1000;$i++) { isset($small_array[\u0026#39;key1\u0026#39;]); } $end_small = hrtime(true); $small_isset_time = ($end_small - $start_small) / 1000 ; echo \u0026#34;Small isset loop is $small_isset_time ns \\n \u0026#34;; $start_small = hrtime(true); for($i = 0;$i \u0026lt; 1000;$i++) { array_key_exists(\u0026#39;key1\u0026#39;, $small_array); } $end_small = hrtime(true); $small_isset_time = ($end_small - $start_small) / 1000 ; echo \u0026#34;Small array_key_exists loop is $small_isset_time ns \\n\u0026#34;; // Medium Loops\n$medium_array = array(); for($i = 0;$i \u0026lt; 1000;$i++) { array_push($medium_array, $i); } $start_medium = hrtime(true); for($i = 0;$i \u0026lt; 1000;$i++) { isset($medium_array[\u0026#39;key1\u0026#39;]); } $end_medium = hrtime(true); $medium_isset_time = ($end_medium - $start_medium) / 1000 ; echo \u0026#34;Medium isset loop is $small_isset_time ns \\n \u0026#34;; $start_medium = hrtime(true); for($i = 0;$i \u0026lt; 1000;$i++) { array_key_exists(\u0026#39;key1\u0026#39;, $medium_array); } $end_medium = hrtime(true); $medium_isset_time = ($end_medium - $start_medium) / 1000 ; echo \u0026#34;medium array_key_exists loop is $medium_isset_time ns \\n\u0026#34;; // Large Loops\n$large_array = array(); for($i = 0;$i \u0026lt; 100000;$i++) { array_push($large_array, $i); } $start_large = hrtime(true); for($i = 0;$i \u0026lt; 1000;$i++) { isset($large_array[\u0026#39;key1\u0026#39;]); } $end_large = hrtime(true); $large_isset_time = ($end_large - $start_large) / 1000 ; echo \u0026#34;Large isset loop is $small_isset_time ns \\n \u0026#34;; $start_large = hrtime(true); for($i = 0;$i \u0026lt; 1000;$i++) { array_key_exists(\u0026#39;key1\u0026#39;, $large_array); } $end_large = hrtime(true); $large_isset_time = ($end_large - $start_large) / 1000 ; echo \u0026#34;Large array_key_exists loop is $large_isset_time ns \\n\u0026#34;; Results: # Small isset loop is 18.179 ns Small array_key_exists loop is 30.321 n medium isset loop is 16.552 ns medium array_key_exists loop is 20.224 ns large isset loop is 16.669 ns large array_key_exists loop is 19.662 ns Usage # Does using one or the other matter? It depends and probably doesn\u0026rsquo;t matter much in my specific use case (where the calls to Lucene/ES are of a larger concern), but, given the null safety guarantees and marginal performance improvement, isset would be better to use in general.\n"},{"id":3,"href":"/docs/languages/php/arrays/","title":"is_set","section":"PHP","content":" Is set versus array_key_exists # I dug into the performance of isset versus array_key_exists in PHP.\nBoth look at an array and determine if it has a specific key, but their behavior is different.\nisset will return false if the value of that key is null. array_key_exists will only look at the key itself.\nUsing an id as an example:\n$a = array(\u0026#39;1\u0026#39; =\u0026gt; \u0026#39;`12345678`\u0026#39;, \u0026#39;key2\u0026#39; =\u0026gt; null); isset($a[\u0026#39;key1\u0026#39;]); //true isset($a[\u0026#39;key2\u0026#39;]); //false array_key_exists(\u0026#39;key1\u0026#39;, $a); // true array_key_exists(\u0026#39;key2\u0026#39;, $a); //true So if you want your function to be null-safe, isset is always the best best. It\u0026rsquo;s also marginally better for performance because it\u0026rsquo;s a PHP language construct rather than a function, like [array_key_exists].\nLooking at a small sample of data, we can see the marginal performance improvements at scale: (note, it\u0026rsquo;s better not to do all three performance comparisons in the same script because, using the same array for inserts means there\u0026rsquo;s a small cache warm-up benefit for array_key_exists if it comes after isset.\n\u0026lt;?php // Small Loops $small_array = array(); for($i = 0;$i \u0026lt; 100;$i++) { array_push($small_array,$i); } $start_small = hrtime(true); for($i = 0;$i \u0026lt; 1000;$i++) { isset($small_array[\u0026#39;key1\u0026#39;]); } $end_small = hrtime(true); $small_isset_time = ($end_small - $start_small) / 1000 ; echo \u0026#34;Small isset loop is $small_isset_time ns \\n \u0026#34;; $start_small = hrtime(true); for($i = 0;$i \u0026lt; 1000;$i++) { array_key_exists(\u0026#39;key1\u0026#39;, $small_array); } $end_small = hrtime(true); $small_isset_time = ($end_small - $start_small) / 1000 ; echo \u0026#34;Small array_key_exists loop is $small_isset_time ns \\n\u0026#34;; // Medium Loops\n$medium_array = array(); for($i = 0;$i \u0026lt; 1000;$i++) { array_push($medium_array, $i); } $start_medium = hrtime(true); for($i = 0;$i \u0026lt; 1000;$i++) { isset($medium_array[\u0026#39;key1\u0026#39;]); } $end_medium = hrtime(true); $medium_isset_time = ($end_medium - $start_medium) / 1000 ; echo \u0026#34;Medium isset loop is $small_isset_time ns \\n \u0026#34;; $start_medium = hrtime(true); for($i = 0;$i \u0026lt; 1000;$i++) { array_key_exists(\u0026#39;key1\u0026#39;, $medium_array); } $end_medium = hrtime(true); $medium_isset_time = ($end_medium - $start_medium) / 1000 ; echo \u0026#34;medium array_key_exists loop is $medium_isset_time ns \\n\u0026#34;; // Large Loops\n$large_array = array(); for($i = 0;$i \u0026lt; 100000;$i++) { array_push($large_array, $i); } $start_large = hrtime(true); for($i = 0;$i \u0026lt; 1000;$i++) { isset($large_array[\u0026#39;key1\u0026#39;]); } $end_large = hrtime(true); $large_isset_time = ($end_large - $start_large) / 1000 ; echo \u0026#34;Large isset loop is $small_isset_time ns \\n \u0026#34;; $start_large = hrtime(true); for($i = 0;$i \u0026lt; 1000;$i++) { array_key_exists(\u0026#39;key1\u0026#39;, $large_array); } $end_large = hrtime(true); $large_isset_time = ($end_large - $start_large) / 1000 ; echo \u0026#34;Large array_key_exists loop is $large_isset_time ns \\n\u0026#34;; Results: # Small isset loop is 18.179 ns Small array_key_exists loop is 30.321 n medium isset loop is 16.552 ns medium array_key_exists loop is 20.224 ns large isset loop is 16.669 ns large array_key_exists loop is 19.662 ns Usage # Does using one or the other matter? It depends and probably doesn\u0026rsquo;t matter much in my specific use case (where the calls to Lucene/ES are of a larger concern), but, given the null safety guarantees and marginal performance improvement, isset would be better to use in general.\n"},{"id":4,"href":"/docs/languages/sql/legible-sql/","title":"Legible SQL","section":"SQL","content":"Writing legible code is seen as the hallmark of a competent developer, and for good reason. If you can\u0026rsquo;t read what you wrote, you can\u0026rsquo;t change what you wrote. There are lots of organizations, especially once you get to larger companies where programming isn\u0026rsquo;t the core competency or value center, that are prime examples of this. The result is a mess that not only impacts the code, but the entire company or business.\nThere are great essays and books on reproducible code in almost every imaginable compiled and interpreted language, from Python, to Java, and great generalist essays, like Knuth\u0026rsquo;s on writing reproducible code.\nBut what about SQL, still the #1 data language of choice in 80% of the tech enterprise world?\nThere are a couple problems with making SQL legible # SQL is often the redheaded stepchild of computer languages.\nFirst, because it\u0026rsquo;s a declarative language, it\u0026rsquo;s hard to do the same kind of concepts that are super-easy in almost any other language: loops, recursion, functions, or even declaring variables. You can do these things in T-SQL, but learning T-SQL and optimizing queries in it is almost not worth it if you just need to pull x dataset for sampling and want to spend most of your time in Python or R analyzing said data, or Y dataset with a,b,and c business logic requirements built in. As a result, it\u0026rsquo;s hard to see SQL as logical and amenable to logical constraints or exceptions, as in \u0026ldquo;regular\u0026rdquo; languages. The other issue is that SQL is often seen as a necessary evil and developers often try to force their way through it in order to write ORMs instead of seeing it as a value-add for data teams.\nSecond, because it doesn\u0026rsquo;t need to be written in a specific format, saved to an editor, or checked into repositories, SQL snippets often end up languishing in Oracle windows, Evernotes, text files in random project directories, and even in emails (guilty as charged -I\u0026rsquo;ve used Outlook as a SQL query storage mechanism before.)\nEven though there is a lot of hype about NoSQL data systems, most data scientists will tell you that they spend a LOT of time getting data sets ready in MySQL or Oracle or SQL Server or Postgres as part of their data flow.\nThen these code snippets are usually stored away in everything from Word docs, to Evernotes, to text files in project directories. Because SQL is declarative and run one person at a time, most times, unless it\u0026rsquo;s being run as part of a Python or R call through the respective APIs, it\u0026rsquo;s not even put into version control.\nBut what about when SQL code needs to be stored or rerun? How to read the jumbled mess of nested subqueries and tons of case statements necessitated by the fact that the business logic for the data requires it?\nHere are a couple tips from painful personal experience. All of these are guidelines, by the way, and geared towards data scientists/analysts working alone on manual SQL tasks that are not part of an automated data flow (but even in that case, cleaning up code will make other components easier to read, too)\nThe guidelines don\u0026rsquo;t mean you have to use them every time. I\u0026rsquo;ve personally written some pretty ugly, functional SQL when it was crunch time. But they will make your life prettier and easier. This is not specifically about optimizing SQL, which I\u0026rsquo;ll cover in another post.\nHow to make SQL legible to other humans # 1. Make all declarative keywords their own new line if possible because it\u0026rsquo;s easier to read and understand which table and which columns are being referenced: # SELECT * FROM tablex WHERE variabley=z AND variablea=12 AND variableb=15 is more legible than\nselect * from tablex where variabley=z AND variablea=12 AND variableb=15 2. Make all declarative statements and DB functions uppercase # SELECT * FROM tablex WHERE variabley=z AND variablea=12 AND variableb=15 ORDER BY RAND() LIMIT 1 is more legible than\nselect * from tablex where variabley=z AND variablea=12 AND variableb=15 order by RAND() limit 1 It\u0026rsquo;s easier to read all uppercase. Or, alternatively, if you don\u0026rsquo;t have time (and it is kind of annoying to remember), make them all lowercase. Just don\u0026rsquo;t mix cases. And uppercase is preferable in any kind of documentation. The same is true for tablenames. Make them either all lower case or all uppercase. Lowercase is preferable to avoid mixing them up with declaratives.\n3. Try to limit the amount of subqueries in your query. # Yes, subqueries are much, much faster if you\u0026rsquo;re aggregating across multiple data sets. Check out this post for more detail as to why. But when you get to the point where you have more than 4 subqueries, you probably want to do some optimization, or if possible, rebuild your tables in a way that makes more sense. If you find yourself CONSTANTLY having to write subqueries, it\u0026rsquo;s a sign that your data schemas are architected incorrectly.\n4. Include comments. # Both in the beginning of the code snippet to say what data the snippet pulls, the business logic it includes, and throughout, as much as possible, to reference subqueries. SQL makes it hard to read through an entire piece of code without running specific pieces one by one to get where you\u0026rsquo;re going and comments will help jog the memory. Include them at the end of lines in code, or in blocks at the beginning of code.\n/*Pulls a sample of 1000 random Nutella orders by month from the Northeast and excludes 16-oz sized jars. Orders without monetary transactions are excluded, too. Pulls into R script for regression analysis. */\tSELECT * from trans a --transactions table JOIN region b --region table with 5 splits ON a.trans_id=b.regionid WHERE a.product_type=\u0026#39;nutella\u0026#39; AND b.regionid=\u0026#39;NE\u0026#39; --NE=northeast ORDER BY RAND() --selecting a random sample of 100 LIMIT 100 5. Use short but concise table aliases # You\u0026rsquo;re probably doing a lot of joins. You want to see what those joins are doing. If you have a transactions_table, you don\u0026rsquo;t want to alias it as transactions_table because that becomes too cluttered. Call it trans. t is too short and probably won\u0026rsquo;t mean anything when you try to read the code back and translate it into business logic.\n6. Use indentation. If you have multiple subqueries, it\u0026rsquo;s much easier to read where the subqueries came from. # SELECT * FROM tablex JOIN (SELECT a, b FROM tabley WHERE x=\u0026#39;5\u0026#39;) ysub WHERE variabley=z AND variablea=12 AND variableb=15 ORDER BY RAND() LIMIT 1 is more legible than\nselect * from tablex JOIN (select a, b from tabley where x=\u0026#39;5\u0026#39;) ysub where variabley=z AND variablea=12 AND variableb=15 order by RAND() limit 1 7. Make new tables. # If you find your query becoming longer than some random threshold you set for yourself, i.e. 15 lines or so AND you find yourself running it over and over, it\u0026rsquo;s probably time to create a permanent table for your data.\nHere\u0026rsquo;s another great post I referenced while writing this one:\nHow do I make complex SQL queries easier to write? on Stack Overflow "},{"id":5,"href":"/docs/languages/python/strptime/","title":"strptime and strftime","section":"Python","content":" One weird trick about Python date formats # I\u0026rsquo;ve been working with date manipulation in Python as part of this Reddit challenge, which asks the programmer to find the year(s) in history when the most US presidents were alive given each president\u0026rsquo;s birth and death date.\nUsually, when working with dates and times in programming, it\u0026rsquo;s best to convert to object types that inherently have some assumptions about time and date already built in to allow for easy addition, subtraction, and date conversion.\nIn Python, the datetime library provides convenient ways to work with date and time data, as well as converting data to and from those formats.\nThere used to be a module called time that was not based exclusively on object-oriented patterns. Datetime is an update of time.\nDatetime has several object types, (as seen in the Python REPL):\n\u0026gt;\u0026gt;\u0026gt;import datetime \u0026gt;\u0026gt;\u0026gt; dir(datetime) [\u0026#39;MAXYEAR\u0026#39;, \u0026#39;MINYEAR\u0026#39;, \u0026#39;__doc__\u0026#39;, \u0026#39;__file__\u0026#39;, \u0026#39;__name__\u0026#39;, \u0026#39;__package__\u0026#39;, \u0026#39;date\u0026#39;, \u0026#39;datetime\u0026#39;, \u0026#39;datetime_CAPI\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;timedelta\u0026#39;, \u0026#39;tzinfo\u0026#39;] I was particularly interested in coverting all the years to datetime or date objects and working with them that way.\nA datetime object looks like this, where you\u0026rsquo;re given back year-month-day-hour-minute-second-microcecond.\n\u0026gt;\u0026gt;\u0026gt;datetime.datetime.now() \u0026gt;\u0026gt;\u0026gt;datetime.datetime(2016, 3, 9, 16, 32, 12, 385142) \u0026gt;\u0026gt;\u0026gt;now = datetime.datetime.now() \u0026gt;\u0026gt;\u0026gt; type(now) \u0026gt;\u0026gt;\u0026gt;\u0026lt;type \u0026#39;datetime.datetime\u0026#39;\u0026gt; A date object is very similar, but does not include the time. \u0026gt;\u0026gt;\u0026gt;nowday = datetime.date.today() \u0026gt;\u0026gt;\u0026gt; nowday \u0026gt;\u0026gt;\u0026gt;datetime.date(2016, 3, 9) To convert strings to these types of object, Python\u0026rsquo;s datetime has strptime, a function that has an equivalent in most modern languages.\nI always get strptime and strftime confused, so here are silly mnemonics I can remember:\nstrptime - converts strings to datetime objects (strips strings to datetime) strftime - converts datetime objects to specific human-readable format while still maintaining their datetimey-ness. (F is for formatting) So let\u0026rsquo;s say I have a date that\u0026rsquo;s a string:\nx = \u0026#34;19 Aug 2015\u0026#34; type(x) \u0026lt;type \u0026#39;str\u0026#39;\u0026gt; And I want to read it into a datetime object:\ndatetime.datetime.strptime(x, \u0026#39;%d %b %Y\u0026#39;) \u0026gt;\u0026gt;\u0026gt;datetime.datetime(2015, 8, 19, 0, 0) Then I can add days, years, subtract days, etc. The ability to automatically do math with differing units of time is what makes this module so convenient.\nI can then spit that calculation back out in any format I want with strftime, for example, I wanted to see the entire month:\n\u0026gt;\u0026gt;\u0026gt; y = datetime.datetime.strptime(x, \u0026#39;%d %b %Y\u0026#39;) \u0026gt;\u0026gt;\u0026gt; datetime.datetime.strftime(y, \u0026#39;%B-%d-%Y\u0026#39;) \u0026#39;August-19-2015\u0026#39; There are a couple caveats that make strftime and strptime really annoying.\nStrptime and formatting: # The first is that the formatting of the input string needs to match exactly, otherwise the assignment throws an exception. This was from when I omitted the spaces from the formatting paramter:\nValueError: time data '19 Aug 2015' does not match format '%d%b%Y' This can be extremely irritating if you have data in the same column that changes formats. In this case, you have to write try/except blocks or find some way of working around this \u0026ldquo;limitation.\u0026rdquo;\nStrftime and Python 2/3: # The other VERY annoying thing is that strptime does not work on data earlier than 1900.\nI came across one when I was trying to add and subtract historical dates of presidents born before 1900. You get the following:\nValueError: year=1601 is before 1900; the datetime strftime() methods require year \u0026gt;= 1900 The documentation states, and I\u0026rsquo;m guessing it\u0026rsquo;s a carryover from C that is no longer relevant to currnent computing memory constraints. Excel has the same issue.\nThe exact range of years for which strftime() works also varies across platforms. Regardless of platform, years before 1900 cannot be used.\nThe easy fix for this is to use Python 3, which accounts for it:\nHere\u0026rsquo;s my simple script, datetimenew.py:\nfrom datetime import datetime x = \u0026#34;19 Aug 1500\u0026#34; y = datetime.strptime(x, \u0026#39;%d %b %Y\u0026#39;) outformat = datetime.strftime(y, \u0026#39;%Y\u0026#39;) print (outformat) If I try to run it in Python 2: vboykis$ python datetimenew.py Traceback (most recent call last): File \u0026#34;datetimenew.py\u0026#34;, line 5, in \u0026lt;module\u0026gt; outformat = datetime.strftime(y, \u0026#39;%Y\u0026#39;) ValueError: year=1500 is before 1900; the datetime strftime() methods require year \u0026gt;= 1900 And 3:\nvboykis$ python3 datetimenew.py 1500 ```python Yet another reason to finally make that switch ;). Or to use isoformat from date instead: ```python date.isoformat() Return a string representing the date in ISO 8601 format, ‘YYYY-MM-DD’. For example, date(2002, 12, 4).isoformat() == \u0026#39;2002-12-04\u0026#39; "},{"id":6,"href":"/docs/languages/python/venv/","title":"venv","section":"Python","content":" Creating a new venv # source\nWhosoever makes a static webpage that just has two things on it:\n1. Unix/Mac/Windows command for creating a new venv environment\n2. Unix/Mac/Windows command for activating new venv environment\nwill achieve super-high search rankings almost instantaneously.\n\u0026mdash; Vicki (@vboykis) October 8, 2019 Instructions # cd project_folder virtualenv venvname source venvname/bin/activate "},{"id":7,"href":"/docs/recsys/","title":"Recsys","section":"Docs","content":" weight: 2 title: Recsys Syllabus # Books\nMMDM Practical Recommender Systems Recommener Systems, Aggarwal Practical Recommender SystemsFalk System design\nReading Google Intro to Recsys Course Recsys Algorithms\nCollaborative Filtering Reading:Amazon Paper Collaborative Filter Survey Paper Matrix Factorization - Reading: Netflix Prize paper Content Filtering - Twitter Paper Hybrid Recommenders: Collaborative + Content Based Methods\nPersonalization Multiarmed Bandits Embeddings Overview and Overview Random Walk - Pixie Definitions of similarity/distance in personalized algos Candidate Generation Process Candidate Ranking Process Recsys Evaluation Metrics:\nPrecision and Recall Overview Overview Overview Precision/Recall/AUC Deep Learning Recsys\n[tbd] Softmax\nRecsys in Production\nSpark ALS Elasticsearch LTR Netflix and Scala for ML Realtime Recs at Netflix Netflix Similariy "},{"id":8,"href":"/docs/platforms/cloud/","title":"Cloud","section":"Platforms","content":" Cloud versus on-prem # A rising concern as companies continue moving infrastructure to the cloud is vendor-lock in.\nFor example, I recently came across a post from 2017 discussing the impact of Lambdas locking you into the AWS ecosystem.\n\u0026ldquo;It\u0026rsquo;s code that tied not just to hardware – which we\u0026rsquo;ve seen before – but to a data center, you can\u0026rsquo;t even get the hardware yourself. And that hardware is now custom fabbed for the cloud providers with dark fiber that runs all around the world, just for them. So literally the application you write will never get the performance or responsiveness or the ability to be ported somewhere else without having the deployment footprint of Amazon.\u0026rdquo;\nIt\u0026rsquo;s true that there is something unnerving about putting all of your trust in a microservic-ey piece of code that runs in a given, custom run-time, with architectures that are hard to reverse-engineer, and take platform-specific triggers (PDF), like S3 calls, as input events.\nThat\u0026rsquo;s a fair assessment. But evaluating serverless functionality on its own, without looking at the alternative path, is not entirely fair. Let\u0026rsquo;s say you\u0026rsquo;re trying to solve the problem of cleaning up log files (by, for example, anonymizing or hashing customer IDs) that land on a minute-by-minute basis in S3 and puts them in a different S3 bucket (or other, vendor-neutral object storage location).\nYou can write a Lambda function that processes them and puts them in a different bucket. The vendor-neutral alternatives would be something like:\nSetting up a Spark streaming application to process the files and put them in S3 or another file system. Creating a complicated bash script that runs (potentially with the help of a scheduler like Airflow) Setting up a Kafka streams application Each of these is vendor-independent (i.e. can run on an EC2/EMR instance, or in your own data center), but each also offers its own unique set of challenges, and locks you into that architecture moving forward. It\u0026rsquo;s possible your company won\u0026rsquo;t want to be on AWS in 2-3 years, and you\u0026rsquo;ll be stuck trying to figure out how to migrate Lambdas to a different architecture. But it\u0026rsquo;s also possible for Kafka or Spark features to break, for your Python program to become too inefficient, for Java 11 features to break on Java 12, and so on.\nOn-prem is a lock-in. Cloud is a lock-in. Every single language you program in is a type of lock-in. Python is easy to get started with, but soon you run into packaging issues and are optimizing the garbage collector. Scala is great, but everyone winds up migrating away from it. And on and on.\nEvery piece of code written in a given language or framework is a step away from any other language, and five more minutes you\u0026rsquo;ll have to spend migrating it to something else. That\u0026rsquo;s fine. You just have to decide what you\u0026rsquo;re willing to be locked into.\nPeople try to hedge these bets by designing truly platform-agnostic, flexible applications. But, it takes a long time to design a truly generic solution, because humans are terrible at long-term forecasting trends. Who could have predicted that Hadoop only has a lifecycle for 3-4 years? Who could have foreseen the shift from SAS into R? Who foresaw the spiraling growth of the JavaScript community? It\u0026rsquo;s hard to design resilient applications for needs 6 months into the future, let alone something that will run in five years.\nCode these days becomes obsolete so quickly, regardless of what\u0026rsquo;s chosen. By the time your needs change, by the time the latest framework is obsolete, all of the code will be rotten anyway. Maybe the Postgres database is still going strong, but Node isn\u0026rsquo;t in favor anymore. AWS and Google Cloud aren\u0026rsquo;t going anywhere over the next 5-10 years.\nThe most dangerous feature about these articles examining cloud lock-in is that they introduce a kind of paralysis into teams that result in applications never being completely fleshed out or finished.\nIn contrast to earlier decades, today we are cursed with an overabundance of technologies that mean that any project starts with a complete evaluation of what makes the most sense. It\u0026rsquo;s possible to mess up. Even the best senior developers make bad architecture and vendor choices. But the good ones also know that, in order to understand whether something is a bad choice, they need to make the current process work.\nIt\u0026rsquo;s possible to spend hours in architecture debates and reading the pros and cons on HN, but what counts most is having something working that you can evaluate. So, write your applications, spin up your Lambdas, keep an ear to the ground, but go to production first.\n"},{"id":9,"href":"/docs/platforms/kubernetes/","title":"Kubernetes","section":"Platforms","content":" Kubernetes Cheat Sheet # What is it? Kubernetes is just a bunch of Docker containers that are abstracted away from you and that you operate on in a group.\nFrequently used shortcuts:\n// see clusters available to you kubectl config view // pick cluster kubectl config use-context dca-production // which namespaces do we have kubectl get namespace | wc -l 65 // how many pods do we have running in our namespace? kubectl -n namespace get pods | wc -l 190 // All current allocated pods kubectl get pods --all-namespaces | wc -l 4038 // pods with container names per spec kubectl get pods -n namespace -o jsonpath=\u0026#34;{.items[*].spec.containers[*].image}\u0026#34; |\\ tr -s \u0026#39;[[:space:]]\u0026#39; \u0026#39;\\n\u0026#39; |\\ sort |\\ uniq -c "},{"id":10,"href":"/docs/platforms/","title":"Platforms","section":"Docs","content":" HI # "},{"id":11,"href":"/docs/languages/python/","title":"Python","section":"Languages","content":" My first love # "},{"id":12,"href":"/docs/languages/scala/","title":"Scala","section":"Languages","content":" Scala! # "},{"id":13,"href":"/docs/languages/sql/","title":"SQL","section":"Languages","content":" SQL # People are always surprised when I say that I use PHP as a machine learning enginer. I\u0026rsquo;d say on any given week, about 30-35% of my time is spent using PHP to serve recommendations in the front-end of the app, and if you work end-to-end with recommender systems, or any machine learning system that gets served as part of a web app, chances are you\u0026rsquo;ll work heavily with the backend language serving it, too.\n"},{"id":14,"href":"/docs/platforms/cloud/aws/","title":"Aws","section":"Cloud","content":" Lambdas # asdfsafdasdfsd\n"},{"id":15,"href":"/docs/recsys/minhash/","title":"Implementing Minhash","section":"Recsys","content":" Chapter 3 of Mining Massive DataSets # Chapter Slides\nMotivating the Chapter\nWe cover the first part of this chapter, which deals with Jaccard similarity, shingling, and minhash.\nOften these days data analysis involves datasets that have high dimensionality, meaning the data set in question has more features than values and to make statistically sound inferences at scale, require large amounts of data (more info here, on p. 22), and it’s these kinds of datasets that Chapter 3 deals with.\nWhat kinds of datasets have these features? In the world of recommendation systems, most any kind of content we’d like to think about recommending, such as text and images, will be high-dimensional spaces. One important ability in dealing with these types of large datasets is to be able to find similar items, in fact it’s this that underlies the principles of offering recommendations, i.e. how similar is this post to this post, how can we recommend things that are similar to other things we know for sure the user likes?\nBefore we recommend something, we have to see whether two items are similar. I personally can eyeball whether two paintings are similar to each other, but when you have to compare millions of pairs of items (for example, 10 million posts per day across 500 million blogs), humans don’t scale.\nJaccard Similarity\nThis chapter of MMDS specifically deals with minhash, one method of combing through millions of items and evaluating how similar they are based on some definition of “distance” between two sets, or groups of items. It was a technique initially used to dedup search results for AltaVista. It can also be used for recommending similar images or detecting plagiarism. It’s mostly used in the context of comparing groups of (text-based) documents. Something that’s important to keep in mind is that we’re not actually looking for the meaning of the sets, just whether these documents are similar on a purely textual level.\nThe chapter starts out by introducing Jaccard similarity, a metric that we can use to determine whether any given two sets of items are similar, for the mathematical definition of set.\nJaccard similarity is the similarity of sets by looking at the relative size of their intersection divided by their union, or SIM(S,T) = |S ∩ T| / |S U T|. So imagine this is two collections of documents: it could be two web pages and the sentences in their pages, or two directories full of pictures, etc.\nHere’s the implementation of Jaccard similarity in Python and Scala.\n{{ }}\nThis is only good for one set of items, though, and doesn’t scale well if the sets are quite large. So we use minhashing, which gets close enough to approximating Jaccard similarity that we can say with confidence that two sets are alike or not alike.\nAs the minhash paper says,\nHowever, for efficient large scale web indexing it is not necessary to determine the actual resemblance value: it suffices to determine whether newly encountered documents are duplicates or near-duplicates of documents already indexed. In other words, it suffices to determine whether the resemblance is above a certain threshold. If this sounds familiar to you, it may be because you’re already familiar with datasketches, the family of probabilistic data structures that create quick glances at a large amount of data and tell you with some degree of certainty that items are the same or not the same, or can introspect a set for certain properties. (Bloom filters and HyperLogLog are examples.)\nShingling\nSo, in order to compare documents, we need to create sets of them that we can fingerprint and compare the fingerprints to each other. In order to create sets of documents, we create a matrix, where the rows are all of the individual elements in the set that we care about and the columns are any given set.\nHere’s a good representation of how this works, from this course. Imagine each set is a single piece of paper with several numbers on it:\nFor word-based documents, though, we need to get at the letter representations of a set. So, instead of using individual numbers, we use shingles, which are really just short strings of any number of letters.\nA document is a string of characters. Define a k-shingle for a document to be\nany substring of length k found within the document. So, Suppose our document D is the string abcdabd, and we pick k = 2. Then the set of 2-shingles for D is {ab, bc, cd, da, bd}. If you’ve ever worked with Pig (sorry), you can think of these as bags, or, in most other languages, tuples. So you get a set of tuples.\nK can be almost any number we want, but at some point, there will be an optimal number where we don’t get a sparse matrix that’s too large to compute or a matrix that’s too small where the similarity between sets is too high. In this way, picking K is similar to picking K for clustering algorithms where you use the highly scientific method of the elbow method until it looks right. Minhash\nOnce we have K, we can set up the matrix. And now we minhash. Here, for example, k is 1.\nFirst, we pick a permutation of rows. What this means in English is that we just randomize the letters until they’re in a different order.\nThen, we create the new matrix, indicating whether each letter is in each set in the same position. For example, if we mix up the letters like this, the rows are b,e,a,d,and c instead of alphabetical order.\nAnd we can see, from the first diagram, that for S1, b is 0, so the value of that index is 0, and so on.\nNow for the actual minhash function, we keep going down the row until we hit the first 1 value. For S1, that value is a:\ncolumn, which is the column for set S1, has 0 in row b, so we proceed to row e the second in the permuted order. There is again a 0 in the column for S1, so we proceed to row a, where we find a 1. Thus. h(S1) = a. And likewise, we see that h(S2) = c, h(S3) = b, and h(S4) = a.\nNow we get to the connection between minhash and Jaccard similarity:\nThe probability that the minhash function for a random permutation of rows produces the same value for two sets equals the Jaccard similarity of those sets This is really important, because it allows us to use Jaccard similarity as a substitute for manually calculating computations between all the rows/columns of a very large document matrix. And,\nMoreover, the more minhashings we use, i.e., the more rows in the signature matrix, the smaller the expected error in the estimate of the Jaccard similarity will be Here’s a good explanation of how this works with a little more detail than MMDS.\nMinhashing Signatures\nThe problem here is that it will take forever to permutate and calculate the Jaccard similarity between all items and sets. So what we do is create a signature, a fingerprint of each set by using\na random hash function that maps row numbers to as many buckets as there\nare rows. Thus, instead of picking n random permutations of rows, we pick n randomly chosen hash functions h1, h2, . . . , hn on the rows. We construct the signature matrix by considering each row in their given order, and then we look across the rows.\nYou can have as many hash functions as you want, but each one will generate a specific number. Get the minimum number of a single hash function, apply across as many as you have, and you’ll get a unique ID for your permutation and then you can compare them across document sets.\nMMDS has a good example of this, but I think this is clearer, and this Python implementation is really, really good at explaining what happens in code.\n"},{"id":16,"href":"/docs/languages/java/","title":"Java","section":"Languages","content":" Java # "},{"id":17,"href":"/docs/languages/php/","title":"PHP","section":"Languages","content":" PHP # People are always surprised when I say that I use PHP as a machine learning enginer. I\u0026rsquo;d say on any given week, about 30-35% of my time is spent using PHP to serve recommendations in the front-end of the app, and if you work end-to-end with recommender systems, or any machine learning system that gets served as part of a web app, chances are you\u0026rsquo;ll work heavily with the backend language serving it, too.\n"},{"id":18,"href":"/docs/what-is-ml/","title":"What is machine learning engineering?","section":"Docs","content":" What is machine learning engineering? # Once, on a crisp cloudless morning in early fall, a machine learning engineer left her home to seek the answers that she could not find, even in the newly-optimized Google results.\nShe closed her laptop, put on her backpack and hiking boots, and walked quietly out her door and past her mailbox, down a dusty path that led past a stream, until the houses around her gave way to broad fields full of ripening corn.\nShe walked past farms where cows grazed peacefully underneath enormous data silos, until the rows of crops gave way to a smattering of graceful pines and oaks, and she found herself in a forest clearing, headed into the woods. She went deeper through the decision trees and finally stopped near a data stream around midday to have lunch and stretch her legs.\nThe sun made its way through the sky and eventually, she walked further, out of the forest. Finally, she found a path that started snaking its way up a mountainside, and she started to hike upwards, through the red rocks. After several hours she stopped and took a drink from her Klean Kanteen as she surveyed the sprawling random forest, the valley spread out below her and the sparkling data lake in the distance.\nHere, on the upper slopes of the mountain, the filepath became erratic and truncated, and now she was mostly climbing, using all of the muscles in her haunches to gain her footing. The sun suddenly lost its heat and shadows started rising in the slopes of the hills.\nFinally, after several more hours, she ascended to the top of the mountain’s gradient. At the peak, there was a low, flat platform. On the platform was a bench, and on the bench, a solitary figure sat, drinking a cold brew and looking out at the sunset.\nShe realized she was in the Presence of a Staff Engineer. Although he could not have been older than thirty-six, his face was already careworn with the wrinkles of a man who had been on PagerDuty more than once in the last fiscal quarter.\n“Oh wise one,” she said, prostrating herself before him and offering him a sacred token of respect, her YubiKey. “I have so many questions about machine learning engineering,” she said.\nThe man looked at her wearily. “This isn’t about that PR from Tuesday, is it? I have it in my Jira backlog, I just haven’t gotten to it yet. That’s why I’m up here. No WiFi means emails don’t exist here.”\n“Oh no, no,” she said. “Far be it from me to Block you, Your Staffness. I have just been a machine learning engineer for a very long time now, since even before Tensorflow 2, and I have been wondering about the meaning of it all.”\nThe man immediately relaxed. “Oh good, at least it’s not a question about bundling Python executables. Come sit down?”\nShe sat near him on the bench.\n“What is it,” he said.\n“Well,” she said, hesitating. “I’ve been doing machine learning engineering for a long time now, and I keep wondering when I’m actually going to get to the\u0026hellip;machine learning engineering.”\n“What do you mean? What do you do on a day-to-day basis?”\n“Last year, my manager tasked me with building a machine learning model to predict the churn for our B2B startup, which specializes in selling mattresses as a service to startups that sell mattresses to B2C consumers.”\n“Ok\u0026hellip;”\n“So I knew I’d need to build a model that would use XGBoost and collect the features we needed to use to predict churn. For example, we’d realized that we needed to know the age of the cohort when the customer signed up because newer customers tended to churn more quickly, how many times they visited our site, how many times they hovered over the “subscribe” button, and how many times they called support.\nBut first, we didn’t even know that customers were churning and our explanatory variable was complete noise, because some of our customers cancelled and then restarted using our service. We’d mark the cancellation in one column, ‘cancelled_service’, and the restart in a second column, ‘new_customer’, so I had to do feature engineering and join two columns from our Salesforce data which we batch via Sqoop into Hive on HDFS on a daily basis. I thought I was good to go, but that day, the batch job failed and so I didn’t have the latest data, so I had to wait.”\n“When the data didn’t hit the next day, I looked at the code responsible for populating those tables,I could check the cron scheduler for the Sqoop job. The cron scheduler for the Sqoop job had somehow become broken (we’re only in the middle of our Airflow migration), and I had to patch it. After a couple hours, we started getting data again and I was able to finally get that response field. Now, I could do some feature engineering to pull the rest of the variables. “\n“The only problem was that, while the billing data was in our legacy HDFS, the newer, clickstream app data, was being streamed via Kinesis into S3, and then into Athena. The other issue with the clickstream data was that it had a ton of non-standard JSON fields that I had to extract specifically to get the events I needed to understand how many times the customer had hovered over the subscribe button and clicked through, and create sessions for user data. I used jq to check those fields and then wrote them up in our metadata dictionary.”\n“Once I had that data available to me, I needed a place that our security team allowed so that I could combine the two data sources. They suggested using Spark on AWS to write out to S3, which they’d then send back to HDFS. Our Spark workloads run on K8s on AWS Spot Instances, but some of the pods were hanging, so I used kubectl to take a look at them and figure out what the issue was.”\nJust realized you can make fake magazines with Canva. pic.twitter.com/Yu0HO8NYmL\n\u0026mdash; Vicki (@vboykis) September 17, 2021 “Now, I had all of the data in a single place, and it was time to build my XGBoost model and iterate on it . But the Python available to me on the machine that could access the HDFS server didn’t have all the dependencies I needed to run XGBoost, so I-”\nThe Staff Engineer held up a palm as if to stop the torrent of pain that flowed from the machine learning engineer. “I’ve heard enough,” he said, sighing.\n“I didn’t even get to the part where I’ll need to surface the serialized results of the churn model into a frontend UI for sales and marketing to consume and make decisions on. The latency-”\nThe machine learning engineer sighed deeply herself and stopped, as if whatever she said next would break her. They sat in silence for a bit. The Staff Engineer sipped his cold brew thoughtfully through a biodegradable straw.\nFinally, the machine learning engineer said, in a very small voice, “I still haven’t gotten to machine learning engineering.”\nThe Staff Engineer scratched his head. “Well, hang on. Let’s list out all of the things that you just said. You said you were:”\nWorking with serialization formats Evaluating modeling requirements and selecting the right model for the business case Using jq and getting what you need from JSON fields Handling cron and bash scripting and getting ETL jobs to run Tuning and optimizing SQL Working on containers and orchestration debugging Shaping, understanding, and describing your data Reasoning about distributed systems Being defensive around data you ingest that impacts your ML pipeline Integration testing across distributed file stores Working with HTTP verbs, networking, simple issues that could go wrong when you’re working across machines, SSH, nohup, screen, port forwarding, exposing ports “And, most importantly, you were\u0026quot;:\nArchitecting a scalable, resilient system to deliver results to key stakeholders using machine learning insights surfaced into an accessible front-end layer “I have some (potentially bad) news for you, you’re doing machine learning engineering.”\n“But I haven’t even touched a model yet.”\n“Have you read the black box paper? The ecosystem of the model is always greater than the model itself.”\nThe machine learning engineer frowned. “But how can it be?”\n“Machine learning systems are new. We’re still in the steam-powered days of machine learning, and yet machine learning is not simply machine learning. It is, at this stage, more engineering than simply machine learning. We’re building more and more on older systems, abstracting away complexity and in the process creating newer and newer levels of it that we now have to manage and hold in our heads. Many of the algorithms have been written. Much of the work we do, both in machine learning, and in development today in general, will be glue work and vendor work.\nKnuth: \u0026quot;I’m worried about the present state of programming. Programmers now are supposed to mostly just use libraries. Programmers aren’t allowed to do their own thing from scratch anymore. They’re supposed to have reusable code that somebody else has written.\n\u0026mdash; Tim Hopper (@tdhopper) April 3, 2019 “So what does this mean for me?”\n“Nothing, keep doing what you’re doing. Keep grinding away. You’ll get to XGBoost eventually, and then, after working with the model for a brief period, you’ll have a whole new set of very boring, non model-specific problems related to:\nFiguring out what online and offline metrics should be, where you\u0026rsquo;ll store them, and how to analyze them Tuning hyperparameters over and over again Cleaning up lots and lots of notebooks Forgetting to shut down your notebooks and incurring cloud costs “So what do I do?” asked the machine learning engineer, distressed. “How do I continue in this field?”\n“Nothing,” the Staff engineer said, leaning back, taking the last sip of his coffee. “You make peace with it. This is the job, writing and gluing together the code that makes drastically different systems speak to each other in data-oriented language.”\n“The beautiful part, though, is when you finally connect all these systems and your database is talking to your streaming platform and your model is reading from the database and you have a new model every day and, finally, one day, someone from sales will come to you and say, we just prevented a customer from churning because we offered them a hypoallergenic organic mattress based on their previous browsing behavior and we kept the customer, and then you can look back through the decision trees, to see the forest of what you have built. The working system in production is our reward, and we always move towards that.”\nThe machine learning engineer looked at the sunset thoughtfully.\nThe Staff Engineer said, “Let me ask you something. Did you enjoy the walk here, even though it was long, hard, and annoying?”\n“Well, yeah,” the MLE paused. “The world is beautiful.”\n“That’s it. In machine learning engineering, the journey, ultimately, is the destination,” the Staff Engineer said, neatly depositing his iced coffee container in the recycling bin located next to the bench, and checked his phone.\n“Shit, PagerDuty,” he exclaimed, and ran to his backpack, swiftly pulling out a MacBook Pro. As he was descending further down the mountain where there was WiFi he turned to the machine learning engineer and said, “See you in prod,” and then he vanished out of view.\n"},{"id":19,"href":"/docs/about/","title":"About Me","section":"Docs","content":" About Me # Hi! I\u0026rsquo;m Vicki. I\u0026rsquo;m a machine learning engineer at Duo working on ML platforms with deep expertise in YAML indentation. I live in Philly with my family. In my free time, I love to write about Life. I also like to think about what technology means in the context of society, and write about that, as well. My main site is here.\nI also love tweeting terrible puns and doodling tech logos.\nNow that I have an iPad with a pencil again, I’m going to take a break from work by creating a series of poorly drawn but honestly named machine learning tool logos. pic.twitter.com/0LhfCXjL80\n\u0026mdash; Vicki (@vboykis) December 16, 2021 "},{"id":20,"href":"/docs/recsys/jaccard-similarity/","title":"Jaccard Similarity","section":"Recsys","content":" Jaccard Similarity # Implemented two different ways:\nimport numpy as numpy import typing a = [1,2,3,4,5,11,12] b = [2,3,4,5,6,8,9] cats = [\u0026#34;calico\u0026#34;, \u0026#34;tabby\u0026#34;, \u0026#34;tom\u0026#34;] dogs = [\u0026#34;collie\u0026#34;, \u0026#34;tom\u0026#34;,\u0026#34;bassett\u0026#34;] def jaccard(list1: list, list2: list)-\u0026gt; float: intersection = len(list(set(list1).intersection(list2))) union = (len(set((list1)) + set(len(list2))) - intersection return float(intersection/union) print(jaccard(cats,dogs)) jaccardSimilarity in Scala # val aVals: Seq[Int] = Seq(1,2,3,4,5,11,12) val bVals: Seq[Int] = Seq(2,3,4,5,6,8,9) def calculateJaccard[T](a: Seq[T], b: Seq[T]): Double = a.intersect(b).size / a.union(b).size.toDouble println(calculateJaccard(aVals, bVals)) "},{"id":21,"href":"/docs/platforms/airflow/","title":"Airflow","section":"Platforms","content":" Airflow Callbacks # "},{"id":22,"href":"/docs/computer-science/hash-aggregate/","title":"Hash aggregates","section":"Computer Science","content":" This post is an expansion of this tweet:\nIf I had to pick a single programming concept where understanding it is like a superpower, it would probably be the hash map (aka in Python, the humble dictionary) because I\u0026#39;ve seen the pattern come up in almost every kind of data/programming work I\u0026#39;ve ever done.\n\u0026mdash; Vicki (@vboykis) July 8, 2020 Hash Aggregate Here # But data work also has its own unique patterns, and I want to talk about one of these that I think is important for all of us to carry around in our back pockets: the humble hash aggregate. The hash aggregate works like this:\nYou have a multidimensional array (or, as us plebes say, table) that contains numerous instances of similar labels. What you want to know is the distinct counts of each category. The implemented algorithm splits the matrix by key and sums the values and then returns the reduced matrix that has only unique keys and the sum values to the user.\nIt\u0026rsquo;s a very simple and ingenious algorithm, and it shows up over and over and over again. If you\u0026rsquo;ve ever done a GROUP BY statement in SQL, you\u0026rsquo;ve used the hash aggregate function. Python\u0026rsquo;s dictionary operations utilize hash aggregates. And so does Pandas\u0026rsquo; split-apply-combine (pictured here from Jake\u0026rsquo;s great post) And, so does Excel\u0026rsquo;s Pivot table function. So does sort filename | uniq -c | sort -nr in Unix. So does the map/reduce pattern that started in Hadoop, and has been implemented in-memory in Spark. An inverted index, the foundation for Elasticsearch (and many search and retrieval platforms) is a hash aggregate.\nSo what? # If you\u0026rsquo;ve worked with either development or data for any length of time, it\u0026rsquo;s almost guaranteed that you\u0026rsquo;ve come across the need to get unique categories of things and then count the things in those categories. In some cases, you might need to build your own implementation of GROUP BY because it doesn\u0026rsquo;t work in your language or framework of choice.\nMy personal opinion is that every data-centric framework that\u0026rsquo;s been around long enough tends to SQL, so everything will eventually implement hash aggregation.\nOnce you understand that hash aggregation is a common pattern, it makes sense to observe it at work, learn more about how to optimize it, and generally think about it.\nOnce we know that this pattern has a name and exists, we have a sense of power over our data work. Confuscius (or whoever attributed the quote to him) once said, “The beginning of wisdom is to call things by their proper name,\u0026quot; and either he was once a curious toddler, or an MLE looking to better understand the history and context of his architecture.\n"},{"id":23,"href":"/docs/platforms/cloud/aws/lambdas/","title":"Lambdas","section":"Aws","content":"asdfsadfsafs\n"},{"id":24,"href":"/docs/recsys/precision-and-recall/","title":"Precision and Recall","section":"Recsys","content":"Precision and Recall\nPrecision and recall are common ways to evaluate the accuracy of your machine learning or information retrieval model. In other contexts, such as statistics, the measurements around these terms is also known as Type I/Type II errors.\nLet’s say you’re working on understanding the relevancy of SnakeSearch, a search engine that looks to find relevant Python documentation for you. Let’s say you want to find documents related to Pandas, as you’re getting started with Pandas, the Python software library, and want some information. You type in “pandas” into SnakeSearch. How good is SnakeSearch’s result set for you?\nYou get back some set of results. Precision is the percent of returned results that are actually good. So we don’t want the results about real pandas, we want the software library. Precision is 3/6, or 1/2 because 3 of the results are relevant to us.\nRecall is what percent of all the possible pandas results out there are returned to you. So let’s say for the case of this example that we have 8 total possible terms. If 3 of those are returned to us, the recall is 3/8.\n"},{"id":25,"href":"/docs/platforms/spark/create-data-python/","title":"Sample data in PySpark","section":"Spark","content":"Here\u0026rsquo;s how to create a small fake dataset for testing in PySpark. More on sc.parallelize.\nfrom pyspark.sql.session import SparkSession rdd = sc.parallelize([(0,None), (0,1), (0,2), (1,2), (1,10), (1,20), (3,18), (3,18), (3,18)]) df=rdd.toDF([\u0026#39;id\u0026#39;,\u0026#39;score\u0026#39;]) df.show() +---+-----+ | id|score| +---+-----+ | 0| null| | 0| 1| | 0| 2| | 1| 2| | 1| 10| | 1| 20| | 3| 18| | 3| 18| | 3| 18| +---+-----+ df.printSchema() root |-- id: long (nullable = true) |-- score: long (nullable = true) None is a special keyword in Python that will let you create nullable fields. If you want to simulate NaN fields, you can do float('nan') for the value. Note that if you don\u0026rsquo;t specify each field as float, you get a null result for the values that are not typed.\nfrom pyspark.sql.session import SparkSession import numpy as np rdd = sc.parallelize([(0,np.nan), (0,float(1)), (0,float(2)), (1,float(2)), (1,float(10)), (1,float(20)), (3,float(18)), (3,float(18)), (3,18)]) df=rdd.toDF([\u0026#39;id\u0026#39;,\u0026#39;score\u0026#39;]) df.show() +---+-----+ | id|score| +---+-----+ | 0| NaN| | 0| 1.0| | 0| 2.0| | 1| 2.0| | 1| 10.0| | 1| 20.0| | 3| 18.0| | 3| 18.0| | 3| null| +---+-----+ "},{"id":26,"href":"/docs/platforms/spark/testing-dataframes/","title":"Writing Unit Tests for Spark Apps in Scala","section":"Spark","content":" Writing Unit Tests for Spark Apps in Scala # Often, something you’d like to test when you’re writing self-contained Spark applications, is whether your given work on a DataFrame or Dataset will return what you want it to after multiple joins and manipulations to the input data.\nThis is not different from traditional unit testing, with the only exception that you\u0026rsquo;d like to test and introspect not only the functionality of the code but the data itself.\nThere’s two ways to be defensive about creating correct data inputs and outputs in Scala Spark. The first by writing and reading from Datasets, which are strongly-typed collections of objects. This works well if you know exactly the data structures you’d like to write.\nIf you’re more in experimental mode, another way to check your data is to write unit tests against Spark code that you can run both locally and as part of CI/CD when you merge your Spark jobs into prod.\nScala Unit Tests # First, a word about unit tests. In Scala, with the Scalatest suite, you can use either traditional TDD unit tests with FunSuite, or FlatSpec, which is more behavior-driven. Flatspec gives you acess to matchers, which are a scala-based DSL of custom assertions.. Scalatest leans towards FlatSpec as the default testing capability in any given Scala project, but you can use either style.\nI\u0026rsquo;ve seen a mix of different styles for Spark, but most of them follow FunSuite, including Spark Test Base, Spark Fast Tests, and this Spark unit testing example library from a previous Spark Summit.\nI\u0026rsquo;ve also chosen to follow FunSuite for this example because I\u0026rsquo;m more familiar with traditional unit testing, and because you can implement much of the functionality, including FlatSpec\u0026rsquo;s matchers, in FunSuite directly.\nHere\u0026rsquo;s an example of a simple test you can set up against a DataFrame:\nimport org.apache.spark.sql.catalyst.expressions.AttributeSet.empty.intersect import org.apache.spark.sql.{DataFrame, Dataset, Encoders, Row, SQLContext, SQLImplicits, SparkSession} import org.apache.spark.{SparkConf, SparkContext} import org.scalatest.{BeforeAndAfterAll, BeforeAndAfterEach, FunSuite} final class YourTestpec extends FunSuite with BeforeAndAfterEach with BeforeAndAfterAll {self =\u0026gt; @transient var ss: SparkSession = null @transient var sc: SparkContext = null private object testImplicits extends SQLImplicits { protected override def _sqlContext: SQLContext = self.ss.sqlContext } import testImplicits._ override def beforeAll(): Unit = { val sparkConfig = new SparkConf() sparkConfig.set(\u0026#34;spark.broadcast.compress\u0026#34;, \u0026#34;false\u0026#34;) sparkConfig.set(\u0026#34;spark.shuffle.compress\u0026#34;, \u0026#34;false\u0026#34;) sparkConfig.set(\u0026#34;spark.shuffle.spill.compress\u0026#34;, \u0026#34;false\u0026#34;) sparkConfig.set(\u0026#34;spark.master\u0026#34;, \u0026#34;local\u0026#34;) ss = SparkSession.builder().config(sparkConfig).getOrCreate() } override def afterAll(): Unit = { ss.stop() } test(\u0026#34;simple dataframe assert\u0026#34;) { val df = spark.createDataFrame(Seq((1,\u0026#34;a string\u0026#34;,\u0026#34;another string\u0026#34;,12344567L) .toDF(\u0026#34;first val\u0026#34;,\u0026#34;stringval\u0026#34;,\u0026#34;stringval2\u0026#34;,\u0026#34;longnum\u0026#34;) assert(df.count == 1) } Note that here I\u0026rsquo;m setting up a Spark session and context beforehand so that when I run sbt test, I\u0026rsquo;m actually running it locally on my machine against the version of Spark that comes bundled with my project. This makes testing quicker than having to ship your project to wherever it runs against your data remotely.\nAnother fantastic alternative is using the Spark Test Base, which has methods for both DataFrames and Datasets and even sets up a SparkContext for you:\nimport org.apache.spark.sql.catalyst.expressions.AttributeSet.empty.intersect import org.apache.spark.sql.{DataFrame, Dataset, Encoders, Row, SQLContext, SQLImplicits, SparkSession} import org.apache.spark.{SparkConf, SparkContext} import org.scalatest.{BeforeAndAfterAll, BeforeAndAfterEach, FunSuite} import com.holdenkarau.spark.testing.{DatasetGeneratorRDDGenerator, SharedSparkContext } final class YourTestpec extends FunSuite with DataFrameSuiteBase with SharedSparkContext with DatasetGenerator{ override def beforeAll(): Unit = { val sparkConfig = new SparkConf() sparkConfig.set(\u0026#34;spark.broadcast.compress\u0026#34;, \u0026#34;false\u0026#34;) sparkConfig.set(\u0026#34;spark.shuffle.compress\u0026#34;, \u0026#34;false\u0026#34;) sparkConfig.set(\u0026#34;spark.shuffle.spill.compress\u0026#34;, \u0026#34;false\u0026#34;) sparkConfig.set(\u0026#34;spark.master\u0026#34;, \u0026#34;local\u0026#34;) ss = SparkSession.builder().config(sparkConfig).getOrCreate() } override def afterAll(): Unit = { ss.stop() } test(\u0026#34;simple dataframe assert\u0026#34;) { val df = spark.createDataFrame(Seq((1,\u0026#34;a string\u0026#34;,\u0026#34;another string\u0026#34;,12344567L) .toDF(\u0026#34;first val\u0026#34;,\u0026#34;stringval\u0026#34;,\u0026#34;stringval2\u0026#34;,\u0026#34;longnum\u0026#34;) val df2 = spark.createDataFrame(Seq((1,\u0026#34;a string\u0026#34;,\u0026#34;another string\u0026#34;,12344567L) .toDF(\u0026#34;first val\u0026#34;,\u0026#34;stringval\u0026#34;,\u0026#34;stringval2\u0026#34;,\u0026#34;longnum\u0026#34;) assertDataFrameEquals(df, df2) } Both Spark Test Base and Fast Tests work well for most of what you\u0026rsquo;d like to test in Spark, such as checking column equality, schemas, totals, and values, and asserting DataFrame equality, which is what I was looking for.\nHeard you like Arrays of Arrays # Sometimes, however, you need to test more complex data structures. For example, often for machine learning, particularly for text processing, you need to create nesting where you might have a DataFrame or Dataset, where the output looks something like:\nuser text_feature_1 text_feature_2 123456789 Array(\u0026ldquo;text\u0026rdquo;,\u0026ldquo;text\u0026rdquo;,\u0026ldquo;text\u0026rdquo;) Map(\u0026ldquo;val\u0026rdquo;-\u0026gt;2, \u0026ldquo;val2\u0026rdquo;-\u0026gt;3) 21234234 Array(\u0026ldquo;text1\u0026rdquo;,\u0026ldquo;text1\u0026rdquo;,\u0026ldquo;text2\u0026rdquo;) Map(\u0026ldquo;val\u0026rdquo;-\u0026gt;4, \u0026ldquo;val2\u0026rdquo;-\u0026gt;5) How do you test for equality between what you expect and what you output here if you\u0026rsquo;re looking to test the entire method generating this DataFrame?\nThe main problem here is in the way Scala does object comparison. Scala by default, in its object comparison methods such as equals, sameElements, and even deep, checks for referential equality, whether these objects are exactly the same object. deepEquals only works on arrays.\nThat means that if your columns are creating complex objects like Maps, they\u0026rsquo;ll never be equal, since, when you create two DataFrames, even if they\u0026rsquo;re equivalent, they\u0026rsquo;re made up of Row objects, which are made up of Maps and Arrays that are each unique new instantiated objects.\nIn this case, you need a way of traversing the data structure.\nHere\u0026rsquo;s what I came up with for my test:\ntest(\u0026#34;test my join\u0026#34;) { // Create our test data val test = spark.DataFrame = ss.createDataFrame(Seq( (12345678901L,Array(\u0026#34;text\u0026#34;,\u0026#34;text\u0026#34;,\u0026#34;text\u0026#34;),Map(\u0026#34;text\u0026#34;-\u0026gt;1 , \u0026#34;text\u0026#34;-\u0026gt;2 , \u0026#34;text\u0026#34; -\u0026gt;2))) .toDF(\u0026#34;user_id\u0026#34;, \u0026#34;my_array\u0026#34;, \u0026#34;my_map\u0026#34;) val expected: DataFrame = spark.createDataFrame(Seq( (12345678901L,Array(\u0026#34;tag\u0026#34;,\u0026#34;tag\u0026#34;,\u0026#34;tag\u0026#34;),Map(\u0026#34;tag\u0026#34;-\u0026gt;1 , \u0026#34;tag\u0026#34;-\u0026gt;2 , \u0026#34;tag\u0026#34; -\u0026gt;2))) .toDF(\u0026#34;user_id\u0026#34;, \u0026#34;my_array\u0026#34;, \u0026#34;my_map\u0026#34;) // Create our test data val expectedArr = test.collect() val testArr = expected.collect() // zip into a collection that compares across tuples of elements (expectedArr zip testArr).foreach{ case (a,b) =\u0026gt; assert(dsEqual(a,b)) } } def dsEqual(a:MyCaseClassRecord, b:MyCaseClassRecord): Boolean ={ a.user_id == b.user_id \u0026amp;\u0026amp; sameArray(a.my_array, b.my_array) \u0026amp;\u0026amp; sameMap(a.my_map,b.my_map) } // compare Arrays with nesting def sameArray(a:Array[String], b:Array[String]) : Boolean ={ a == b || a.sameElements(b) } // compare Maps with nesting def sameMap(a:Map[String,Int], b:Map[String,Int]) : Boolean ={ a == b || a.sameElements(b) } "}]